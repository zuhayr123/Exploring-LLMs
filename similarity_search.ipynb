{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/var/folders/fm/4ph5fzys1_11v_27_wq8pnbc0000gn/T/ipykernel_89616/204070638.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/var/folders/fm/4ph5fzys1_11v_27_wq8pnbc0000gn/T/ipykernel_89616/204070638.py:19: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/opt/miniconda3/envs/llm/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/var/folders/fm/4ph5fzys1_11v_27_wq8pnbc0000gn/T/ipykernel_89616/204070638.py:44: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  classification_chain = LLMChain(\n",
      "/var/folders/fm/4ph5fzys1_11v_27_wq8pnbc0000gn/T/ipykernel_89616/204070638.py:52: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = retrieval_qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalQA answer: Yoga is the hobby of Sarah, as stated in the context that she is \"a 25-year-old yoga enthusiast\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/4ph5fzys1_11v_27_wq8pnbc0000gn/T/ipykernel_89616/204070638.py:61: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  classification_output = classification_chain.run(classification_input).strip().lower()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Classification Output: based on the user's question and the assistant's answer, i would say that the answer addresses the user's question. the user asked for sarah's hobbies, and the assistant provided a specific hobby (yoga) that they claim is associated with sarah.\n",
      "\n",
      "while the assistant's answer might not be entirely comprehensive or definitive (e.g., it doesn't mention any other possible hobbies sarah may have), it provides some relevant information that directly answers the user's question.\n",
      "Classification result: no\n",
      "Final Classification Result: no\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Ensure compatibility with your setup\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize LLM\n",
    "llm = Ollama(model=\"llama3.1\")\n",
    "\n",
    "# Load the text document\n",
    "document_path = 'text_doc.md'  # Replace with your document path\n",
    "loader = TextLoader(document_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Create embeddings and vectorstore\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "retrieval_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Create a prompt for classification\n",
    "classification_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Given the user's question and the assistant's answer, determine whether the assistant's answer addresses the user's question, it is okay even if the answer is only partially correct, as long as it is not completely empty of any information, in such cases start your answer with yes, otherwise no.\n",
    "\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create an LLM chain for classification\n",
    "classification_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=classification_prompt\n",
    ")\n",
    "\n",
    "def classify_answer(question):\n",
    "    try:\n",
    "        # Get answer from RetrievalQA chain\n",
    "        response = retrieval_qa_chain({\"query\": question})\n",
    "        answer = response[\"result\"]\n",
    "        print('RetrievalQA answer:', answer)\n",
    "\n",
    "        # Use the LLM to classify whether the answer contains the information\n",
    "        classification_input = {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "        classification_output = classification_chain.run(classification_input).strip().lower()\n",
    "        print('Raw Classification Output:', classification_output)\n",
    "\n",
    "        # Extract 'yes' or 'no' from the classification output\n",
    "        if 'yes' in classification_output:\n",
    "            classification_result = 'yes'\n",
    "        elif 'no' in classification_output:\n",
    "            classification_result = 'no'\n",
    "        else:\n",
    "            # Default to 'no' if unable to determine\n",
    "            classification_result = 'no'\n",
    "\n",
    "        print('Classification result:', classification_result)\n",
    "        return classification_result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RetrievalQA or classification chain: {e}\")\n",
    "        return 'no'\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input question\n",
    "    question = input(\"Enter your question: \")\n",
    "\n",
    "    # Classify the answer\n",
    "    result = classify_answer(question)\n",
    "\n",
    "    # Output the result\n",
    "    print(f\"Final Classification Result: {result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
